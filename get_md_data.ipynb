{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import timeseries_dataset_from_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MD trajectories\n",
    "\n",
    "filename should be the location where the numpy array is stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cartesian coordinate file. \n",
    "file_name = '/Users/user/Mirror/long_seq_LSTM/extracted_pdb_trajectory_more_config_recentering_aligned.npy'\n",
    "full_data = np.load(file_name)[:,1:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5856, 6546), 'number of atoms: ', 2182.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the data \n",
    "full_data.shape, 'number of atoms: ', full_data.shape[1]/3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the training set and normalise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5856, 6546)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = full_data\n",
    "\n",
    "\n",
    "training_set.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the data to 2D and then normalise the three axis together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5856, 6546), 'max value: 0.9999999999999998 and the min valu 0.0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples, num_features = training_set.shape\n",
    "data_2d = training_set.reshape(num_samples * num_features, 1)\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_2d)\n",
    "\n",
    "data_2d = scaler.transform(data_2d)\n",
    "\n",
    "\n",
    "training_set = data_2d.reshape(num_samples, num_features)\n",
    "\n",
    "\n",
    "training_set.shape, f'max value: {training_set.max()} and the min valu {training_set.min()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 3), (4000, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = training_set[:4000,:3]\n",
    "target = training_set[1000:5000,:3]\n",
    "features.shape, target.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the scaler to rescale the predictions later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the data into timeSeries data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_aromic_dataset(data_x,data_y,seq_length,t_func=timeseries_dataset_from_array,batch_num = 4000):\n",
    "    x = t_func(\n",
    "        data=data_x,\n",
    "        targets=None,\n",
    "        # sequence_stride = seq_length,\n",
    "        sequence_stride = 1,\n",
    "        sequence_length=seq_length,\n",
    "        batch_size = batch_num\n",
    "        \n",
    "    )\n",
    "\n",
    "    y = t_func(\n",
    "        data=data_y,\n",
    "        targets=None,\n",
    "        # sequence_stride = seq_length,\n",
    "        sequence_stride = 1,\n",
    "        sequence_length=seq_length,\n",
    "        batch_size = batch_num\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    for input in x:\n",
    "        print(f'x shape: {input.shape}')\n",
    "        break\n",
    "        \n",
    "\n",
    "    for input in y:\n",
    "        print(f'y shape: {input.shape}')\n",
    "        break\n",
    "\n",
    "    \n",
    "    for batch in zip(x, y):\n",
    "        \n",
    "        inputs, targets = batch\n",
    "        \n",
    "        break\n",
    "\n",
    "    # targets = np.array(targets).reshape(targets.shape[0],(targets.shape[1]*targets.shape[2]))\n",
    "\n",
    "    # print('target and features')\n",
    "    # print(inputs.shape)\n",
    "    # print(targets.shape)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 02:12:42.523176: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-12-19 02:12:42.524223: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (3001, 1000, 3)\n",
      "y shape: (3001, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "features,target = get_time_aromic_dataset(features,target,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3001, 1000, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test short lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 3\n",
    "sequence_length = 1000\n",
    "lstm_dim = 64\n",
    "BATCH_size = 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow packages to train the lstm models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "# from AutoEncoder import AutoEncode_layer_build\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Bidirectional, TimeDistributed, Concatenate\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the lstm model and Compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sequence_length, feature_dim))\n",
    "\n",
    "x = layers.LSTM(lstm_dim, return_sequences=True )(inputs)\n",
    "\n",
    "x = layers.LSTM(lstm_dim, return_sequences=True )(x)\n",
    "\n",
    "output = layers.TimeDistributed(layers.Dense(3, activation='linear'))(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount_factor_input = Input(shape=(1))\n",
    "\n",
    "# # Input for features\n",
    "# features_input = Input(shape=(sequence_length, feature_dim))\n",
    "\n",
    "# # Repeat discount factor along the sequence length\n",
    "# discount_factor_sequence = layers.RepeatVector(sequence_length)(discount_factor_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input for features\n",
    "features_input = Input(shape=(sequence_length, feature_dim), name='features_input')\n",
    "\n",
    "# Input for discount factors\n",
    "discount_factor_input = Input(shape=(sequence_length, 1), name='discount_factor_input')\n",
    "\n",
    "# Merge features and discount factors along the last dimension\n",
    "merged_input = Concatenate(axis=-1)([features_input, discount_factor_input])\n",
    "\n",
    "# # Concatenate features and discount factor along the feature dimension\n",
    "# # combined_input = Concatenate(axis=-1)([features_input, discount_factor_sequence])\n",
    "# combined_input = Concatenate(axis=-1)([features_input, discount_factor_sequence])\n",
    "\n",
    "# LSTM layers\n",
    "x = LSTM(lstm_dim, return_sequences=True)(merged_input)\n",
    "x = LSTM(lstm_dim, return_sequences=True)(x)\n",
    "\n",
    "# TimeDistributed Dense layer for output\n",
    "output = layers.TimeDistributed(Dense(3, activation='linear'))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "features_input (InputLayer)     [(None, 1000, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "discount_factor_input (InputLay [(None, 1000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000, 4)      0           features_input[0][0]             \n",
      "                                                                 discount_factor_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 1000, 64)     17664       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 1000, 64)     33024       lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 1000, 3)      195         lstm_9[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 50,883\n",
      "Trainable params: 50,883\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs = [features_input, discount_factor_input], outputs = output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy','mae',keras.metrics.RootMeanSquaredError()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning using discount factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initial high discount factor for short-term focus\n",
    "initial_discount_factor = 0.9\n",
    "\n",
    "# Gradual decrease in discount factor to emphasize long-term rewards\n",
    "discount_factors = np.linspace(initial_discount_factor, 0.5, sequence_length).reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_discount_factors = np.random.uniform(0, 1, (sequence_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9      ],\n",
       "       [0.8995996],\n",
       "       [0.8991992],\n",
       "       [0.8987988],\n",
       "       [0.8983984]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_factors[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5856"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3001, 1000, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_factors = np.tile(discount_factors, (3001, 1, 1))\n",
    "discount_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "# Define a function to print the discount factor during training\n",
    "def print_discount_factor(epoch, logs, model, features, discount_factors):\n",
    "    # Get the discount factor values from the discount_factors input data\n",
    "    discount_factor_values = discount_factors[:, :, 0]  # Assuming discount_factors has shape (batch_size, sequence_length, 1)\n",
    "    print(\"Current discount factor values:\", discount_factor_values)\n",
    "\n",
    "# Create a LambdaCallback to call the print_discount_factor function\n",
    "discount_factor_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: print_discount_factor(epoch, logs, model, features, discount_factors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 38s 498ms/step - loss: 0.0028 - accuracy: 0.8181 - mae: 0.0401 - root_mean_squared_error: 0.0528 - val_loss: 0.0044 - val_accuracy: 0.9434 - val_mae: 0.0504 - val_root_mean_squared_error: 0.0664\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 36s 487ms/step - loss: 0.0025 - accuracy: 0.8437 - mae: 0.0373 - root_mean_squared_error: 0.0502 - val_loss: 0.0052 - val_accuracy: 0.9700 - val_mae: 0.0555 - val_root_mean_squared_error: 0.0721\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 36s 476ms/step - loss: 0.0022 - accuracy: 0.8549 - mae: 0.0346 - root_mean_squared_error: 0.0471 - val_loss: 0.0058 - val_accuracy: 0.9706 - val_mae: 0.0566 - val_root_mean_squared_error: 0.0759\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 36s 475ms/step - loss: 0.0023 - accuracy: 0.8479 - mae: 0.0353 - root_mean_squared_error: 0.0477 - val_loss: 0.0055 - val_accuracy: 0.9871 - val_mae: 0.0544 - val_root_mean_squared_error: 0.0740\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 38s 501ms/step - loss: 0.0029 - accuracy: 0.8504 - mae: 0.0381 - root_mean_squared_error: 0.0536 - val_loss: 0.0034 - val_accuracy: 0.9980 - val_mae: 0.0472 - val_root_mean_squared_error: 0.0579\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 36s 484ms/step - loss: 0.0062 - accuracy: 0.7562 - mae: 0.0654 - root_mean_squared_error: 0.0790 - val_loss: 0.0083 - val_accuracy: 0.9980 - val_mae: 0.0764 - val_root_mean_squared_error: 0.0913\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 36s 483ms/step - loss: 0.0051 - accuracy: 0.7567 - mae: 0.0570 - root_mean_squared_error: 0.0712 - val_loss: 0.0087 - val_accuracy: 0.7225 - val_mae: 0.0766 - val_root_mean_squared_error: 0.0930\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 36s 485ms/step - loss: 0.0037 - accuracy: 0.7768 - mae: 0.0478 - root_mean_squared_error: 0.0612 - val_loss: 0.0070 - val_accuracy: 0.9447 - val_mae: 0.0675 - val_root_mean_squared_error: 0.0835\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 36s 483ms/step - loss: 0.0046 - accuracy: 0.7917 - mae: 0.0528 - root_mean_squared_error: 0.0677 - val_loss: 0.0075 - val_accuracy: 0.9980 - val_mae: 0.0700 - val_root_mean_squared_error: 0.0868\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 37s 487ms/step - loss: 0.0046 - accuracy: 0.7560 - mae: 0.0531 - root_mean_squared_error: 0.0680 - val_loss: 0.0069 - val_accuracy: 0.7704 - val_mae: 0.0675 - val_root_mean_squared_error: 0.0829\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 37s 489ms/step - loss: 0.0046 - accuracy: 0.7532 - mae: 0.0531 - root_mean_squared_error: 0.0681 - val_loss: 0.0058 - val_accuracy: 0.9522 - val_mae: 0.0625 - val_root_mean_squared_error: 0.0764\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 37s 495ms/step - loss: 0.0051 - accuracy: 0.7565 - mae: 0.0573 - root_mean_squared_error: 0.0714 - val_loss: 0.0074 - val_accuracy: 0.7116 - val_mae: 0.0718 - val_root_mean_squared_error: 0.0859\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 37s 499ms/step - loss: 0.0046 - accuracy: 0.7552 - mae: 0.0530 - root_mean_squared_error: 0.0681 - val_loss: 0.0123 - val_accuracy: 0.4746 - val_mae: 0.0909 - val_root_mean_squared_error: 0.1110\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 37s 497ms/step - loss: 0.0053 - accuracy: 0.7581 - mae: 0.0591 - root_mean_squared_error: 0.0728 - val_loss: 0.0090 - val_accuracy: 0.7676 - val_mae: 0.0782 - val_root_mean_squared_error: 0.0948\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 37s 498ms/step - loss: 0.0048 - accuracy: 0.7554 - mae: 0.0541 - root_mean_squared_error: 0.0690 - val_loss: 0.0095 - val_accuracy: 0.6440 - val_mae: 0.0812 - val_root_mean_squared_error: 0.0976\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 39s 516ms/step - loss: 0.0044 - accuracy: 0.7606 - mae: 0.0514 - root_mean_squared_error: 0.0660 - val_loss: 0.0092 - val_accuracy: 0.6363 - val_mae: 0.0772 - val_root_mean_squared_error: 0.0959\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 38s 505ms/step - loss: 0.0040 - accuracy: 0.7736 - mae: 0.0488 - root_mean_squared_error: 0.0631 - val_loss: 0.0110 - val_accuracy: 0.5931 - val_mae: 0.0808 - val_root_mean_squared_error: 0.1048\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 39s 514ms/step - loss: 0.0039 - accuracy: 0.7704 - mae: 0.0477 - root_mean_squared_error: 0.0621 - val_loss: 0.0104 - val_accuracy: 0.5543 - val_mae: 0.0779 - val_root_mean_squared_error: 0.1021\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 39s 520ms/step - loss: 0.0038 - accuracy: 0.7652 - mae: 0.0476 - root_mean_squared_error: 0.0620 - val_loss: 0.0102 - val_accuracy: 0.6132 - val_mae: 0.0779 - val_root_mean_squared_error: 0.1009\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 39s 518ms/step - loss: 0.0037 - accuracy: 0.7671 - mae: 0.0465 - root_mean_squared_error: 0.0606 - val_loss: 0.0133 - val_accuracy: 0.5363 - val_mae: 0.0908 - val_root_mean_squared_error: 0.1151\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 39s 521ms/step - loss: 0.0037 - accuracy: 0.7828 - mae: 0.0470 - root_mean_squared_error: 0.0610 - val_loss: 0.0074 - val_accuracy: 0.8004 - val_mae: 0.0675 - val_root_mean_squared_error: 0.0863\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 39s 524ms/step - loss: 0.0031 - accuracy: 0.7964 - mae: 0.0429 - root_mean_squared_error: 0.0560 - val_loss: 0.0062 - val_accuracy: 0.9104 - val_mae: 0.0619 - val_root_mean_squared_error: 0.0790\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 39s 521ms/step - loss: 0.0032 - accuracy: 0.7913 - mae: 0.0433 - root_mean_squared_error: 0.0568 - val_loss: 0.0115 - val_accuracy: 0.7813 - val_mae: 0.0871 - val_root_mean_squared_error: 0.1071\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 40s 527ms/step - loss: 0.0029 - accuracy: 0.8023 - mae: 0.0412 - root_mean_squared_error: 0.0541 - val_loss: 0.0065 - val_accuracy: 0.8138 - val_mae: 0.0643 - val_root_mean_squared_error: 0.0806\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 40s 532ms/step - loss: 0.0034 - accuracy: 0.8042 - mae: 0.0442 - root_mean_squared_error: 0.0579 - val_loss: 0.0082 - val_accuracy: 0.8296 - val_mae: 0.0733 - val_root_mean_squared_error: 0.0903\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 39s 519ms/step - loss: 0.0033 - accuracy: 0.7893 - mae: 0.0444 - root_mean_squared_error: 0.0579 - val_loss: 0.0037 - val_accuracy: 0.9897 - val_mae: 0.0486 - val_root_mean_squared_error: 0.0607\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 39s 521ms/step - loss: 0.0025 - accuracy: 0.8277 - mae: 0.0380 - root_mean_squared_error: 0.0502 - val_loss: 0.0054 - val_accuracy: 0.9601 - val_mae: 0.0567 - val_root_mean_squared_error: 0.0738\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 43s 574ms/step - loss: 0.0019 - accuracy: 0.8817 - mae: 0.0327 - root_mean_squared_error: 0.0440 - val_loss: 0.0055 - val_accuracy: 0.9269 - val_mae: 0.0562 - val_root_mean_squared_error: 0.0742\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 39s 525ms/step - loss: 0.0034 - accuracy: 0.8369 - mae: 0.0434 - root_mean_squared_error: 0.0582 - val_loss: 0.0128 - val_accuracy: 0.6352 - val_mae: 0.0883 - val_root_mean_squared_error: 0.1129\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 40s 529ms/step - loss: 0.0028 - accuracy: 0.8449 - mae: 0.0405 - root_mean_squared_error: 0.0532 - val_loss: 0.0094 - val_accuracy: 0.6878 - val_mae: 0.0768 - val_root_mean_squared_error: 0.0971\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 40s 539ms/step - loss: 0.0035 - accuracy: 0.8030 - mae: 0.0452 - root_mean_squared_error: 0.0590 - val_loss: 0.0064 - val_accuracy: 0.8724 - val_mae: 0.0660 - val_root_mean_squared_error: 0.0800\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 40s 534ms/step - loss: 0.0041 - accuracy: 0.7575 - mae: 0.0498 - root_mean_squared_error: 0.0642 - val_loss: 0.0086 - val_accuracy: 0.7074 - val_mae: 0.0723 - val_root_mean_squared_error: 0.0928\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 44s 582ms/step - loss: 0.0029 - accuracy: 0.8063 - mae: 0.0414 - root_mean_squared_error: 0.0541 - val_loss: 0.0061 - val_accuracy: 0.8556 - val_mae: 0.0603 - val_root_mean_squared_error: 0.0778\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 45s 601ms/step - loss: 0.0025 - accuracy: 0.8436 - mae: 0.0382 - root_mean_squared_error: 0.0504 - val_loss: 0.0063 - val_accuracy: 0.8813 - val_mae: 0.0602 - val_root_mean_squared_error: 0.0794\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 45s 604ms/step - loss: 0.0044 - accuracy: 0.7965 - mae: 0.0523 - root_mean_squared_error: 0.0666 - val_loss: 0.0072 - val_accuracy: 0.9469 - val_mae: 0.0693 - val_root_mean_squared_error: 0.0850\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 40s 533ms/step - loss: 0.0040 - accuracy: 0.7886 - mae: 0.0494 - root_mean_squared_error: 0.0636 - val_loss: 0.0061 - val_accuracy: 0.8282 - val_mae: 0.0619 - val_root_mean_squared_error: 0.0780\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 39s 526ms/step - loss: 0.0034 - accuracy: 0.7910 - mae: 0.0448 - root_mean_squared_error: 0.0580 - val_loss: 0.0093 - val_accuracy: 0.7859 - val_mae: 0.0734 - val_root_mean_squared_error: 0.0965\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 41s 553ms/step - loss: 0.0027 - accuracy: 0.8572 - mae: 0.0394 - root_mean_squared_error: 0.0524 - val_loss: 0.0108 - val_accuracy: 0.8367 - val_mae: 0.0865 - val_root_mean_squared_error: 0.1037\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 39s 524ms/step - loss: 0.0033 - accuracy: 0.8221 - mae: 0.0439 - root_mean_squared_error: 0.0575 - val_loss: 0.0090 - val_accuracy: 0.7323 - val_mae: 0.0744 - val_root_mean_squared_error: 0.0947\n",
      "Current discount factor values: [[0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " ...\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]\n",
      " [0.9       0.8995996 0.8991992 ... 0.5008008 0.5004004 0.5      ]]\n",
      "Epoch 40/100\n",
      "68/75 [==========================>...] - ETA: 3s - loss: 0.0024 - accuracy: 0.8921 - mae: 0.0363 - root_mean_squared_error: 0.0489"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit([features, discount_factors], \n\u001b[1;32m      2\u001b[0m           target, \n\u001b[1;32m      3\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[discount_factor_callback],\n\u001b[1;32m      5\u001b[0m           validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/keras/engine/training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([features, discount_factors], \n",
    "          target, \n",
    "          epochs=100,\n",
    "          callbacks=[discount_factor_callback],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3001, 1000, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3001, 1000, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3001, 1000, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1000, 3)]         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1000, 64)          17408     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 1000, 3)           195       \n",
      "=================================================================\n",
      "Total params: 50,627\n",
      "Trainable params: 50,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discount_factor_input = Input(shape=(1,))\n",
    "\n",
    "# Input for features\n",
    "features_input = Input(shape=(sequence_length, feature_dim))\n",
    "\n",
    "# LSTM layers\n",
    "x = LSTM(lstm_dim, return_sequences=True)(features_input)\n",
    "x = LSTM(lstm_dim, return_sequences=True)(x)\n",
    "\n",
    "# TimeDistributed Dense layer for output\n",
    "# output = TimeDistributed(Dense(3, activation='linear'))(x)\n",
    "output = layers.TimeDistributed(layers.Dense(3, activation='linear'))(x)\n",
    "# Model\n",
    "# model2 = tf.keras.Model(inputs=[features_input, discount_factor_input], outputs=output)\n",
    "model2 = tf.keras.Model(inputs=features_input, outputs=output)\n",
    "\n",
    "# Compile the model (adjust optimizer and loss as needed)\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print model summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 4/31 [==>...........................] - ETA: 28s - loss: 0.1543"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model2\u001b[39m.\u001b[39;49mfit(features, \n\u001b[1;32m      2\u001b[0m           target, \n\u001b[1;32m      3\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m           batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/keras/engine/training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/homebrew/Caskroom/miniforge/base/envs/tf_rnn/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.fit(features, \n",
    "          target, \n",
    "          epochs=100,\n",
    "          batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3001, 1000, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f42ed79099961fad288f4906d20c61d1ab8d43abf4fe0c04843b1e8e4728994e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
