{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Keras and tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Build RNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Bidirectional, TimeDistributed\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# creating PDB\n",
    "import csv\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalisation and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get PDB atom postions for all frames \n",
    "DATA_FILE = 'CAM_extracted_pdb_trajectory.npy'\n",
    "Coordinates_array = np.load(DATA_FILE)\n",
    "Coordinates_array = Coordinates_array.astype(float)\n",
    "\n",
    "# Number of Atoms\n",
    "ATOM_NUM = 8076\n",
    "\n",
    "\n",
    "# change to pandas DataFrame and remove the time stamp columns\n",
    "df_coo = pd.DataFrame(Coordinates_array)\n",
    "df_coo_only = df_coo.iloc[:, 1:]\n",
    "\n",
    "# Change back to numpy array that only has\n",
    "cleaned_coo = df_coo_only.to_numpy()\n",
    "\n",
    "\n",
    "# Use MinMax normalisation on the dataset\n",
    "scaler = MinMaxScaler()\n",
    "Coordinates_array_scalled = scaler.fit_transform(cleaned_coo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##              Split data to train validate and test in a 50%, 25%, 25% split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train and Val data (3000, 8076)\n",
      "Shape of Testing data (2000, 8076)\n"
     ]
    }
   ],
   "source": [
    "# Remove the Testing sample\n",
    "\n",
    "# size of training sample\n",
    "train_len = int(Coordinates_array_scalled.shape[0]*0.6)\n",
    "\n",
    "test_len = Coordinates_array_scalled.shape[0] - train_len\n",
    "\n",
    "end_val = train_len\n",
    "\n",
    "tv_Coordinates_array_scalled = Coordinates_array_scalled[:end_val]\n",
    "test_Coordinates_array_scalled = Coordinates_array_scalled[end_val:]\n",
    "\n",
    "print(f'Shape of Train and Val data {tv_Coordinates_array_scalled.shape}')\n",
    "print(f'Shape of Testing data {test_Coordinates_array_scalled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder build and data encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoEncode_layer_build(lst,act_func='lr'):\n",
    "    \"\"\" \n",
    "    builds an Autoencoder along with both the\n",
    "    decode and encode \n",
    "    \"\"\"\n",
    "    # activation function where the defualt is \n",
    "    # leaky relu\n",
    "    if act_func == 'lr':\n",
    "        AF = keras.layers.LeakyReLU(alpha=0.3)\n",
    "\n",
    "    # user set activation function\n",
    "    else:\n",
    "        AF = act_func\n",
    "\n",
    "    # build the encoder layers\n",
    "    # using the denfined list (lst)\n",
    "    for i, layer in enumerate(lst):\n",
    "\n",
    "        if i == 0: # first layer (input layer)\n",
    "            encoder_input = keras.Input((layer,), name='Encoder_input')\n",
    "\n",
    "        elif i ==1: # second layer \n",
    "            layers = keras.layers.Dense(\n",
    "                            layer, activation=AF, name=f'layer_{i}',\n",
    "                            kernel_initializer='normal')(encoder_input)\n",
    "\n",
    "            layers = keras.layers.BatchNormalization(name=f'Batch_Norm_{i}')(layers)\n",
    "\n",
    "\n",
    "        elif i ==(len(lst)-1): # the bottleneck layer\n",
    "\n",
    "            encoder_output = keras.layers.Dense(\n",
    "                            layer, activation=AF, name=f'layer_{i}',\n",
    "                            kernel_initializer='normal')(layers)      \n",
    "\n",
    "        else: #other layers between second layer and final layer\n",
    "            layers = keras.layers.Dense(\n",
    "                            layer, activation=AF, name=f'layer_{i}',\n",
    "                            kernel_initializer='normal')(layers)\n",
    "            \n",
    "            layers = keras.layers.BatchNormalization(name=f'Batch_Norm_{i}')(layers)\n",
    "\n",
    "    # build the encoder \n",
    "    encoder = keras.Model(encoder_input, encoder_output)\n",
    "\n",
    "    # build the decoder layer\n",
    "    for i, layer in enumerate(lst[::-1]):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        elif i == 1:\n",
    "            de_layer_input = keras.layers.Dense(layer, activation=AF,\n",
    "                                name=f'de_layer_{i}',\n",
    "                                kernel_initializer='normal')(encoder_output)\n",
    "\n",
    "            de_layer_input = keras.layers.BatchNormalization(name=f'Batch_Norm_d_{i}')(de_layer_input)\n",
    "\n",
    "        elif i ==2:\n",
    "            de_layer = keras.layers.Dense(\n",
    "                       layer, activation=AF, name=f'de_layer_{i}', \n",
    "                       kernel_initializer='normal')(de_layer_input)\n",
    "\n",
    "            de_layer = keras.layers.BatchNormalization(name=f'Batch_Norm_d_{i}')(de_layer)\n",
    "        \n",
    "        elif i == (len(lst)-1):\n",
    "            decode_output = keras.layers.Dense(layer, activation=AF, name=f'de_layer_{i}',\n",
    "                                          kernel_initializer='normal')(de_layer)\n",
    "\n",
    "        else:\n",
    "            de_layer = keras.layers.Dense(layer, activation=AF, name=f'de_layer_{i}',\n",
    "                                          kernel_initializer='normal')(de_layer)\n",
    "\n",
    "            de_layer = keras.layers.BatchNormalization(name=f'Batch_Norm_d_{i}')(de_layer)\n",
    "\n",
    "    \n",
    "    AE = keras.Model(encoder_input, decode_output)\n",
    "    # get the number of layer in Decoder\n",
    "    loop = len(lst) + (len(lst)-2)\n",
    "    print(loop)\n",
    "    # Build decoder\n",
    "    for i in range(loop):\n",
    "        if i == 0:\n",
    "            dec_input = keras.Input((lst[-1],), name='Encded_input')\n",
    "        \n",
    "        elif i == 1:\n",
    "            decco = AE.layers[-(loop-i)](dec_input)\n",
    "        \n",
    "        else:\n",
    "            decco = AE.layers[-(loop-i)](decco)\n",
    "         \n",
    "\n",
    "\n",
    "    decoder = keras.Model(dec_input, decco, name='Decoder_lr')\n",
    "\n",
    "    return encoder,decoder, AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8076, 519, 260, 120, 8, 1]\n",
      "10\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder_input (InputLayer)   [(None, 8076)]            0         \n",
      "_________________________________________________________________\n",
      "layer_1 (Dense)              (None, 519)               4191963   \n",
      "_________________________________________________________________\n",
      "Batch_Norm_1 (BatchNormaliza (None, 519)               2076      \n",
      "_________________________________________________________________\n",
      "layer_2 (Dense)              (None, 260)               135200    \n",
      "_________________________________________________________________\n",
      "Batch_Norm_2 (BatchNormaliza (None, 260)               1040      \n",
      "_________________________________________________________________\n",
      "layer_3 (Dense)              (None, 120)               31320     \n",
      "_________________________________________________________________\n",
      "Batch_Norm_3 (BatchNormaliza (None, 120)               480       \n",
      "_________________________________________________________________\n",
      "layer_4 (Dense)              (None, 8)                 968       \n",
      "_________________________________________________________________\n",
      "Batch_Norm_4 (BatchNormaliza (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "layer_5 (Dense)              (None, 1)                 9         \n",
      "_________________________________________________________________\n",
      "de_layer_1 (Dense)           (None, 8)                 16        \n",
      "_________________________________________________________________\n",
      "Batch_Norm_d_1 (BatchNormali (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "de_layer_2 (Dense)           (None, 120)               1080      \n",
      "_________________________________________________________________\n",
      "Batch_Norm_d_2 (BatchNormali (None, 120)               480       \n",
      "_________________________________________________________________\n",
      "de_layer_3 (Dense)           (None, 260)               31460     \n",
      "_________________________________________________________________\n",
      "Batch_Norm_d_3 (BatchNormali (None, 260)               1040      \n",
      "_________________________________________________________________\n",
      "de_layer_4 (Dense)           (None, 519)               135459    \n",
      "_________________________________________________________________\n",
      "Batch_Norm_d_4 (BatchNormali (None, 519)               2076      \n",
      "_________________________________________________________________\n",
      "de_layer_5 (Dense)           (None, 8076)              4199520   \n",
      "=================================================================\n",
      "Total params: 8,734,251\n",
      "Trainable params: 8,730,623\n",
      "Non-trainable params: 3,628\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Shape of AE layers\n",
    "lst2 = [8076, 519, 260, 120, 8, 1]\n",
    "print(lst2)\n",
    "\n",
    "# define the AutoEncoder layers\n",
    "encoder_1, decoder_1, AE_1 = AutoEncode_layer_build(lst2) \n",
    "AE_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Autoencoder\n",
    "opt = keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "AE_1.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "\n",
    "history = AE_1.fit(tv_Coordinates_array_scalled, tv_Coordinates_array_scalled,\n",
    "                   epochs=10, batch_size=8, validation_split=0.25, verbose=0)\n",
    "\n",
    "# Encode the entire data\n",
    "\n",
    "# Train data\n",
    "encoded_train = []\n",
    "for i in range(tv_Coordinates_array_scalled.shape[0]):\n",
    "    encoded_coordinate = list(encoder_1.predict(tv_Coordinates_array_scalled[i].reshape((-1, ATOM_NUM)))[0])\n",
    "    encoded_train.append(encoded_coordinate)\n",
    "\n",
    "\n",
    "# test data\n",
    "encoded_test = []\n",
    "for i in range(test_Coordinates_array_scalled.shape[0]):\n",
    "    encoded_coordinate = list(encoder_1.predict(\n",
    "        test_Coordinates_array_scalled[i].reshape((-1, 8076)))[0])\n",
    "    encoded_test.append(encoded_coordinate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Target for the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert both the test and training datasets to DataFrames\n",
    "df_encoded_train = pd.DataFrame(encoded_train)\n",
    "df_encoded_test = pd.DataFrame(encoded_test)\n",
    "\n",
    "# create a 1000 frame by 1000 frame\n",
    "df_train_features = df_encoded_train.iloc[:1000]\n",
    "df_train_target = df_encoded_train.iloc[1000:2000]\n",
    "\n",
    "# create target for val data \n",
    "df_val_features = df_encoded_train.iloc[2000:2500]\n",
    "df_val_target = df_encoded_train.iloc[2500:3000]\n",
    "\n",
    "# target data for test data\n",
    "df_test_feature = df_encoded_test[:1000]\n",
    "df_test_target = df_encoded_test[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise data again with MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_2 = MinMaxScaler()\n",
    "\n",
    "# scale training dataset\n",
    "train_features_saclled = scaler_2.fit_transform(df_train_features.to_numpy())\n",
    "train_target_scalled = scaler_2.fit_transform(df_train_target.to_numpy())\n",
    "\n",
    "# scale valadation dataset\n",
    "val_features_scalled = scaler_2.fit_transform(df_val_features.to_numpy())\n",
    "val_target_scalled = scaler_2.fit_transform(df_val_target.to_numpy())\n",
    "\n",
    "# scale testing dataset\n",
    "test_feature_scalled = scaler_2.fit_transform(df_test_feature.to_numpy())\n",
    "test_target_scalled = scaler_2.fit_transform(df_test_target.to_numpy())\n",
    "\n",
    "\n",
    "# prepare the Dataset for time series ML\n",
    "# by reshaping the dataset \n",
    "\n",
    "\n",
    "# # train\n",
    "# train_features_saclled = train_features_saclled.reshape(10,100,1)\n",
    "# train_target_scalled = train_target_scalled.reshape(10,100,1)\n",
    "\n",
    "# # val\n",
    "# val_features_scalled = val_features_scalled.reshape(5,100,1)\n",
    "# val_target_scalled = val_target_scalled.reshape(5,100,1)\n",
    "\n",
    "# # test\n",
    "# test_feature_scalled = test_feature_scalled.reshape(10,100,1)\n",
    "# test_target_scalled = test_target_scalled.reshape(10,100,1)\n",
    "\n",
    "\n",
    "# train\n",
    "train_features_saclled = train_features_saclled.reshape(1, 1000,1)\n",
    "train_target_scalled = train_target_scalled.reshape(1, 1000,1)\n",
    "\n",
    "# val\n",
    "val_features_scalled = val_features_scalled.reshape(1, 500, 1)\n",
    "val_target_scalled = val_target_scalled.reshape(1, 500, 1)\n",
    "\n",
    "# test\n",
    "test_feature_scalled = test_feature_scalled.reshape(1, 1000, 1)\n",
    "test_target_scalled = test_target_scalled.reshape(1, 1000, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_saclled.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Coordinates_array\n",
    "del Coordinates_array_scalled\n",
    "del df_test_feature\n",
    "del df_test_target\n",
    "del df_train_features\n",
    "del df_train_target\n",
    "del test_Coordinates_array_scalled\n",
    "del tv_Coordinates_array_scalled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden_LSTM_Layer_1 (Bidirec (None, 1000, 256)         133120    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000, 256)         0         \n",
      "_________________________________________________________________\n",
      "Hidden_LSTM_Layer_2 (Bidirec (None, 1000, 256)         394240    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 256)         0         \n",
      "_________________________________________________________________\n",
      "Output_Layer (TimeDistribute (None, 1000, 1)           257       \n",
      "=================================================================\n",
      "Total params: 527,617\n",
      "Trainable params: 527,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_RNN = Sequential()\n",
    "\n",
    "# create input layer\n",
    "model_RNN.add(Input(shape=(train_features_saclled.shape[1],train_features_saclled.shape[2]), name='Input_layer'))\n",
    "\n",
    "model_RNN.add(Bidirectional(LSTM(128, activation='relu',return_sequences=True,),name='Hidden_LSTM_Layer_1'))\n",
    "model_RNN.add(Dropout(0.2))\n",
    "\n",
    "model_RNN.add(Bidirectional(LSTM(128, activation='relu',return_sequences=True,),name='Hidden_LSTM_Layer_2'))\n",
    "model_RNN.add(Dropout(0.2))\n",
    "\n",
    "model_RNN.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output_Layer')) \n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the RNN\n",
    "opt = keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model_RNN.compile(optimizer=opt, loss='mse', metrics='mse')\n",
    "\n",
    "model_RNN.fit(train_features_saclled, train_target_scalled,\n",
    "              batch_size=1,\n",
    "              epochs=1000,\n",
    "              validation_data=(val_features_arr_reshaped,\n",
    "                               val_targets_arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model and export PDB for PyMol analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
